bots:
  # Import default some rules
  - import: (data)/bots/_deny-pathological.yaml
  - import: (data)/bots/aggressive-brazilian-scrapers.yaml
  - import: (data)/meta/ai-block-aggressive.yaml
  - import: (data)/crawlers/internet-archive.yaml
  - import: (data)/common/keep-internet-working.yaml

  # Allow curl and wget
  - name: allow-curl-and-wget
    user_agent_regex: '^(curl|Wget)'
    action: ALLOW

  # Browsers
  - name: browser
    user_agent_regex: '^Mozilla/5\.0'
    action: CHALLENGE
    challenge:
      algorithm: fast
      difficulty: 5
      report_as: 5

  # Block everything else
  - name: block-everything-else
    action: DENY
    user_agent_regex: '.*'

# By default, send HTTP 200 back to clients that either get issued a challenge
# or a denial. This seems weird, but this is load-bearing due to the fact that
# the most aggressive scraper bots seem to really, really, want an HTTP 200 and
# will stop sending requests once they get it.
status_codes:
  CHALLENGE: 200
  DENY: 200

store:
  backend: bbolt
  parameters:
    path: /srv/data/anubis.bdb

thresholds:
  - name: everything
    expression: "weight == weight"
    action: CHALLENGE
    challenge:
      algorithm: fast
      difficulty: 6
      report_as: 6
